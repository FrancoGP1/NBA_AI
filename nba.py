# -*- coding: utf-8 -*-
"""NBA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G3-fS9GlTdB9q7s1FZCUPmi3xzn0B8Ch

Librarie declaration
"""

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os

"""Import Data"""

# Dataset is now stored in a Pandas Dataframe
df = pd.read_csv('/content/drive/My Drive/TEC_9_Sem/Sistemas Inteligentes/all_seasons.csv')

#See how many instances we have
df.shape

df.info()

"""We can see that we have 11145 players data across 23 seasons and 22 features."""

df.describe()

"""This dataset contains all of player records from 1996 season. We can't include all players in this list, as some of them maybe drafted far back beyond 1996 season, thus the record in this dataset does not count all of their accomplishment."""

df.drop(df[df.draft_year<'1995'].index, inplace=True)

"""We also need to clean some weird number, such as a player that drafted in 82nd pick, and turn all of undrafted player to 61st pick for analytics purpose"""

df['draft_number'].replace('Undrafted','82',inplace=True)
df['draft_number'].replace('82','61',inplace=True)
df['draft_number'] = pd.to_numeric(df['draft_number'])

"""Then we want to discard some players that only played few games for their entire career. First we need to list them then drop them from the original dataframe"""

df_player = df[['player_name','gp']].groupby('player_name').sum().reset_index()
df_player = df_player.loc[df_player['gp'] < 5]
for p in df_player['player_name']:
    df.drop(df[df.player_name==p].index, inplace=True)

"""We remove all the things that don't have numbers"""

# Removing Non-integer features.
df2=df.drop(['team_abbreviation', 'college', 'country', 'draft_year', 'draft_round', 'draft_number', 'season', 'oreb_pct', 'dreb_pct', 'usg_pct','ts_pct','ast_pct'], axis = 1)

df2.info()

"""We create a new object without names"""

df3 = df2.drop(['player_name', 'Unnamed: 0'], axis = 1) 
df3.info()

"""# Correlation Matrix"""

df3.corr()

import seaborn as sns
sns.pairplot(df3)

fig,((ax1,ax2,ax3,ax4),(ax5,ax6,ax7,ax8))=plt.subplots(2,4,figsize=(30,10))
ax=[ax1,ax2,ax3,ax4,ax5,ax6,ax7]
k=0

for col in df3.columns:
    if col!='gp':
        ax[k].scatter(df3[col],df3['gp'])
        ax[k].set_xlabel(col)
        ax[k].set_ylabel("gp")
        k=k+1

"""# Hypothesis"""

from sklearn.metrics import r2_score
X_m=['age','player_height','player_weight','net_rating','pts','reb','ast']
y_m=['gp']

Xm_train,Xm_test,ym_train,ym_test=train_test_split(df3[X_m],df3[y_m])

mul_model=LinearRegression().fit(Xm_train,ym_train)
print('training multiple feature model score:',mul_model.score(Xm_train,ym_train),'\ntesting multiple feature model score:',mul_model.score(Xm_test,ym_test),
      '\ntraining multiple feature model r2 score:',r2_score(ym_train,mul_model.predict(Xm_train)),'\ntesting multiple feature model r2 score:',r2_score(ym_test,mul_model.predict(Xm_test)))

#cross validation
from sklearn.model_selection import cross_val_score
cv_score=cross_val_score(mul_model,df3[X_m],df3[y_m])
print('cross validation score:',cv_score)
print('mean cross validation score:',np.mean(cv_score))

#Polynomial Regression
from sklearn.preprocessing import PolynomialFeatures

poly_X_m=PolynomialFeatures(degree=4).fit_transform(df3[X_m])
y_m=['gp']

poly_X_train,poly_X_test,y_trainn,y_testt=train_test_split(poly_X_m,df3[y_m])

poly_model= LinearRegression().fit(poly_X_train,y_trainn)

print('training polynomial model score:',poly_model.score(poly_X_train,y_trainn),'\ntesting polynomial model score:',poly_model.score(poly_X_test,y_testt),
      '\ntraining polynomial model r2 score:',r2_score(y_trainn,poly_model.predict(poly_X_train)),'\ntesting polynomial model r2 score:',r2_score(y_testt,poly_model.predict(poly_X_test)))

#Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor

tree_model=  DecisionTreeRegressor(max_depth=7).fit(Xm_train,ym_train)

print('training decision tree model score:',tree_model.score(Xm_train,ym_train),'\ntesting decision tree model score:',tree_model.score(Xm_test,ym_test),
      '\ntraining decision tree model r2 score:',r2_score(ym_train,tree_model.predict(Xm_train)),'\ntesting decision tree model r2 score:',r2_score(ym_test,tree_model.predict(Xm_test)))

#KNN Regression
from sklearn.neighbors import KNeighborsRegressor
knn_model=KNeighborsRegressor(n_neighbors=6000).fit(Xm_train,ym_train)
print('training knn model score:',knn_model.score(Xm_train,ym_train),'\ntesting knn model score:',knn_model.score(Xm_test,ym_test),
      '\ntraining knn model r2 score:',r2_score(ym_train,knn_model.predict(Xm_train)),'\ntesting knn model r2 score:',r2_score(ym_test,knn_model.predict(Xm_test)))

#Neural network
 from sklearn.neural_network import MLPRegressor
 
 regr = MLPRegressor(random_state=1, max_iter=10000).fit(Xm_train, ym_train)
print('training neural model score:',regr.score(Xm_train,ym_train),'\ntesting neural model score:',regr.score(Xm_test,ym_test),
      '\ntraining neural model r2 score:',r2_score(ym_train,regr.predict(Xm_train)),'\ntesting neural model r2 score:',r2_score(ym_test,regr.predict(Xm_test)))
 #regr.score(Xm_test, ym_test)

"""# New Hypothesis"""

from sklearn.metrics import r2_score
from sklearn import preprocessing
X2_m=['pts','reb','ast']
y2_m=['gp']

Xm1_train,Xm1_test,ym1_train,ym1_test=train_test_split(df3[X2_m],df3[y2_m])

min_max_scaler = preprocessing.MinMaxScaler()
Xm2_test =  preprocessing.scale(Xm1_test)
Xm2_train =  preprocessing.scale(Xm1_train)
ym2_train = min_max_scaler.fit_transform(ym1_train)
ym2_test = min_max_scaler.fit_transform(ym1_test)


#Xm2_train = quantile_transformer.fit_transform(Xm1_train)
#Xm2_test = quantile_transformer.transform(Xm1_test)

mul_model=LinearRegression().fit(Xm2_train,ym2_train)
print('training multiple feature model score:',mul_model.score(Xm2_train,ym2_train),'\ntesting multiple feature model score:',mul_model.score(Xm2_test,ym2_test),
      '\ntraining multiple feature model r2 score:',r2_score(ym2_train,mul_model.predict(Xm2_train)),'\ntesting multiple feature model r2 score:',r2_score(ym2_test,mul_model.predict(Xm2_test)))

#cross validation
from sklearn.model_selection import cross_val_score
cv_score=cross_val_score(mul_model,df3[X2_m],df3[y2_m])
print('cross validation score:',cv_score)
print('mean cross validation score:',np.mean(cv_score))

#Polynomial Regression
from sklearn.preprocessing import PolynomialFeatures

poly_X_m=PolynomialFeatures(degree=3).fit_transform(df3[X_m])
y_m=['gp']

poly_X_train,poly_X_test,y_trainn,y_testt=train_test_split(poly_X_m,df3[y_m])

poly_model= LinearRegression().fit(poly_X_train,y_trainn)


print('training polynomial model score:',poly_model.score(poly_X_train,y_trainn),'\ntesting polynomial model score:',poly_model.score(poly_X_test,y_testt),
      '\ntraining polynomial model r2 score:',r2_score(y_trainn,poly_model.predict(poly_X_train)),'\ntesting polynomial model r2 score:',r2_score(y_testt,poly_model.predict(poly_X_test)))

#Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor

tree_model=  DecisionTreeRegressor(max_depth=6).fit(Xm_train,ym_train)

print('training decision tree model score:',tree_model.score(Xm_train,ym_train),'\ntesting decision tree model score:',tree_model.score(Xm_test,ym_test),
      '\ntraining decision tree model r2 score:',r2_score(ym_train,tree_model.predict(Xm_train)),'\ntesting decision tree model r2 score:',r2_score(ym_test,tree_model.predict(Xm_test)))

#Neural network
 from sklearn.neural_network import MLPRegressor
 
 regr = MLPRegressor(random_state=1, max_iter=10000).fit(Xm2_train, ym2_train)

print('training neural model score: ', regr.score(Xm2_train,ym2_train),'\ntesting neural model score:',regr.score(Xm2_test,ym2_test),
      '\ntraining neural model r2 score:',r2_score(ym2_train,regr.predict(Xm2_train)),'\ntesting neural model r2 score:',r2_score(ym2_test,regr.predict(Xm2_test)))
 #regr.score(Xm_test, ym_test)

